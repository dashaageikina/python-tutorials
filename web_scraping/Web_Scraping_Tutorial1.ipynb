{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491b946f",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial - Python Basics\n",
    "**Dasha Ageikina, Ph.D.**\n",
    "\n",
    "Glynmoran\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com/in/dariaageikina/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5e9f07",
   "metadata": {},
   "source": [
    "## We‚Äôll cover:\n",
    "- The basics of web scraping  \n",
    "- 3 web scraping examples in Python  \n",
    "  - You can do web scraping in R too, but I recommend learning Python because it is:  \n",
    "    - a universal language for most industry data jobs  \n",
    "    - easy to debug and troubleshoot and has many good resources online  \n",
    "    - more efficient than R  \n",
    "- Cookies and other considerations in web scraping  \n",
    "- Challenges  \n",
    "\n",
    "> **Disclaimer:** This is not a comprehensive tutorial on web scraping since it‚Äôs a very big topic. I focus on what was useful for me. There may be better/more efficient solutions for some projects ‚Äì please share if you find those!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba773211",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "- **Web scraping** ‚Äì automated process of extracting data from websites.  \n",
    "- **HTTP (HyperText Transfer Protocol)** ‚Äì protocol used by the web to transfer data between clients (your browser or Python program) and servers (website hosts).  \n",
    "- **URL (Uniform Resource Locator)** ‚Äì web address (link to the website).  \n",
    "- **HTML (HyperText Markup Language)** ‚Äì standard language used to create websites.  \n",
    "- **CSS (Cascading Style Sheets)** ‚Äì language for formatting HTML elements (colors, fonts, etc.)\n",
    "\n",
    "> To see a website in HTML using Google Chrome:  \n",
    "> - Mac: `View ‚Üí Developer ‚Üí Developer Tools`  \n",
    "> - Windows: `Three dots (‚ãÆ) ‚Üí More tools ‚Üí Developer Tools`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13510251",
   "metadata": {},
   "source": [
    "## Some HTML Tags\n",
    "\n",
    "```html\n",
    "<html>   <!-- The root element wrapping the entire HTML document -->\n",
    "<head>   <!-- Meta-information like title, links to scripts/styles -->\n",
    "<body>   <!-- All visible content of the web page -->\n",
    "<nav>    <!-- Navigation menus -->\n",
    "<div>    <!-- Generic container -->\n",
    "<table>  <!-- Tabular data -->\n",
    "<tr>     <!-- Table row -->\n",
    "<td>     <!-- Table cell -->\n",
    "<a>      <!-- Hyperlink -->\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf0865",
   "metadata": {},
   "source": [
    "## The Main Web Scraping Steps in Python\n",
    "\n",
    "1. Send an HTTP request (GET) to the URL and save the response.  \n",
    "2. This object contains the HTML content of the site ‚Äì it has a complex structure.  \n",
    "3. Parse the HTML using libraries like:\n",
    "   - `lxml`\n",
    "   - `BeautifulSoup`\n",
    "   - `html5lib`\n",
    "4. Sometimes, you need to interact with the website using **Selenium**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e8632",
   "metadata": {},
   "source": [
    "## ‚úÖ Problem 1 ‚Äì Download Data from Files with Links\n",
    "\n",
    "Sometimes we can download data directly using the file links, like [here](https://files.cdpr.ca.gov/pub/outgoing/pur_archives/)\n",
    "\n",
    "Let's write a program that downloads the files **pur1974.zip** and **pur1975.zip** for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2477fa3",
   "metadata": {},
   "source": [
    "## üí° Problem 1 ‚Äì Python Solution\n",
    "\n",
    "1. If you haven't installed requests library yet, do that first by running \"pip install requests\". Do this for other libraries that we will use below: lxml, re, bs4, selenium, time.\n",
    "2. Assign a string to PATH_TO_YOUR_FOLDER with the path to YOUR folder where you want the files to be downloaded.\n",
    "3. Execute the code and check that the files are in your folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de62383d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://files.cdpr.ca.gov/pub/outgoing/pur_archives/pur1974.zip...\n",
      "Downloading https://files.cdpr.ca.gov/pub/outgoing/pur_archives/pur1975.zip...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "PATH_TO_YOUR_FOLDER = \"/Users/dariaageikina/Downloads\"\n",
    "\n",
    "#Loop over years\n",
    "for year in range(1974, 1976):\n",
    "    url = f\"https://files.cdpr.ca.gov/pub/outgoing/pur_archives/pur{year}.zip\"\n",
    "    filename = f\"{PATH_TO_YOUR_FOLDER}/pur{year}.zip\"\n",
    "    \n",
    "    print(f\"Downloading {url}...\")\n",
    "    \n",
    "    response = requests.get(url) #connect to the link\n",
    "    #write content of the link into a new file\n",
    "    with open(filename, \"wb\") as f: #wb means write the file into a binary mode - an option for non-txt files\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1270ce06",
   "metadata": {},
   "source": [
    "## ‚úÖ Problem 2 ‚Äì Parse No-Link Elements from a Website\n",
    "\n",
    "We want to collect data on pesticide type of [this pesticide](https://apps.cdpr.ca.gov/cgi-bin/label/labrep.pl?fmt=1&63069=on) \n",
    "\n",
    "We need our program to say it's a miticide and an insecticide.\n",
    "\n",
    "Steps in Chrome:\n",
    "1. Open the link in Chrome  \n",
    "2. Right-click the element ‚Üí Inspect  \n",
    "3. Right-click the code ‚Üí Copy ‚Üí ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbaf73",
   "metadata": {},
   "source": [
    "## üí° Problem 2 ‚Äì Python Solution 1 (XPath + lxml)\n",
    "- Copy the **full XPath** (the address of the element).\n",
    "- Best when extracting a single element (one table in our case) with a clear XPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77134184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MITICIDE', 'INSECTICIDE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import html\n",
    "import re\n",
    "\n",
    "url = 'https://apps.cdpr.ca.gov/cgi-bin/label/labrep.pl?fmt=1&63069=on'\n",
    "response = requests.get(url)\n",
    "\n",
    "#convert raw HTML content into tree-like structure that we can parse\n",
    "tree = html.fromstring(response.content)\n",
    "#include your xpath found manually through Google Chrome Developer\n",
    "xpath = '/html/body/div/main/div/div[2]/div[1]/table/tbody'\n",
    "#get the element that we need\n",
    "element = tree.xpath(xpath)\n",
    "\n",
    "#convert extracted element into text\n",
    "pesticide_types = element[0].text_content().strip()\n",
    "re.findall(r'\\b[A-Z]{3,}\\b', pesticide_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84489542",
   "metadata": {},
   "source": [
    "## üí° Problem 2 ‚Äì Python Solution 2 (Selectors + BeautifulSoup)\n",
    "\n",
    "- Use tags/selectors when extracting many elements from different parts of HTML.\n",
    "- Requires more HTML structure investigation.\n",
    "\n",
    "In our case, it happens so that we are interested in the first table on the page\n",
    "- `soup.find(\"table\")` ‚Üí a command for getting the first table on the page. Similarly, `soup.find(\"a\")` would get us the first hyperlink on the page.\n",
    "- Extract all elements and pick relevant ones (in our case, elements #1 and #3, since Python starts counting from 0, and elements #0 and #2 would be \"O0\" and \"N0\").\n",
    "\n",
    "If we needed to extract all tables from the website, we would run `soup.find_all(\"table\")`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c5a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MITICIDE\n",
      "INSECTICIDE\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://apps.cdpr.ca.gov/cgi-bin/label/labrep.pl?fmt=1&63069=on'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('table') #we are lucky because we need the first table\n",
    "entries = table.find_all('td')\n",
    "print(entries[1].text.strip())\n",
    "print(entries[3].text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c0ae8f",
   "metadata": {},
   "source": [
    "## ‚úÖ Problem 3 ‚Äì Interact with Elements on the Website\n",
    "\n",
    "We want the program to:\n",
    "- Enter `63069` into a search bar [here](https://apps.cdpr.ca.gov/docs/label/epanum.cfm)\n",
    "- Press the ‚ÄúSubmit‚Äù button and get us to the next webpage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8f9a1",
   "metadata": {},
   "source": [
    "## üí° Problem 3 ‚Äì Python Solution with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6e458b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "#open the browser and the website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://apps.cdpr.ca.gov/docs/label/epanum.cfm\")\n",
    "\n",
    "# Wait for the page to fully load\n",
    "time.sleep(5)\n",
    "\n",
    "# Locate the input field (through Chrome inspection)\n",
    "zip_input = driver.find_element(By.NAME, \"p_epas\")\n",
    "zip_input.clear()\n",
    "zip_input.send_keys(\"63069\")\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Press the submit button\n",
    "submit_button = driver.find_element(By.XPATH, \"/html/body/div/main/div/div[2]/form/input[3]\")\n",
    "submit_button.click()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af788ac8",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "\n",
    "- Mainly used for web testing  \n",
    "- Can find elements by: ID, name, XPath, URL, URL name, tag name, class name, CSS selector, etc.  \n",
    "- You can:\n",
    "  - Press buttons\n",
    "  - Fill in forms with text\n",
    "  - Scroll, drag and drop, navigate forward and back\n",
    "  - Wait for elements to load  \n",
    "\n",
    "Good [resource](https://selenium-python.readthedocs.io/index.html) to learn more: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871aede0",
   "metadata": {},
   "source": [
    "## Additional Considerations\n",
    "\n",
    "Examples above are very basic. In reality, we would also need to:\n",
    "- Add clauses to handle errors\n",
    "- Add a browser user agent to mimic regular user behavior in a browser  \n",
    "  - Check out the [list of user-agent strings](https://deviceatlas.com/blog/list-of-user-agent-strings)\n",
    "- Pauses between requests ‚Äì to avoid being blocked for inundating the website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba30b9",
   "metadata": {},
   "source": [
    "## Cookies\n",
    "\n",
    "- Store session info and user preferences  \n",
    "- Websites use them to track sessions and recognize bots  \n",
    "- Without proper handling, you may be logged out or see different content than expected\n",
    "- Setting proper cookies can help mimic a real browser\n",
    "\n",
    "üìñ [Learn more about cookies](https://scrape.do/blog/web-scraping-cookies/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cfbc8",
   "metadata": {},
   "source": [
    "## üí° Problem 2 ‚Äì Python Solution 1 (XPath + lxml) - UPDATED\n",
    "If I were to account for some of the considerations above, my script would change to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e0f66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MITICIDE', 'INSECTICIDE']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import re\n",
    "\n",
    "url = 'https://apps.cdpr.ca.gov/cgi-bin/label/labrep.pl?fmt=1&63069=on'\n",
    "\n",
    "# Custom headers with User-Agent, change yours to the one compatible with your device\n",
    "# Mine is MacBook M3, but the user agent for it is the same as for Intel Mac, so I use this one\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15'\n",
    "    '(KHTML, like Gecko) Version/18.3.1 Safari/605.1.15'\n",
    "}\n",
    "\n",
    "# Use session to handle cookies automatically\n",
    "session = requests.Session()\n",
    "session.headers.update(headers)\n",
    "\n",
    "response = session.get(url)\n",
    "\n",
    "# Check if HTTP response is OK\n",
    "if response.status_code == 200:\n",
    "    tree = html.fromstring(response.content)\n",
    "    xpath = '/html/body/div/main/div/div[2]/div[1]/table/tbody'\n",
    "    element = tree.xpath(xpath)\n",
    "\n",
    "    if element:\n",
    "        pesticide_types = element[0].text_content().strip()\n",
    "        pesticide_types = re.findall(r'\\b[A-Z]{3,}\\b', pesticide_types)\n",
    "        print(pesticide_types)\n",
    "    else:\n",
    "        print(\"Could not find the target element using XPath.\")\n",
    "else:\n",
    "    print(f\"Request failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf81b899",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "- **Anti-bot systems** may block your IP address, usually temporarily\n",
    "  - Don'd scrape too much too quickly\n",
    "  - Adjust all parameters to mimic browser behavior \n",
    "  - Limit the number of requests during the day\n",
    "  - Consider using this [library](https://github.com/ultrafunkamsterdam/undetected-chromedriver) (make sure you do not violate any laws first)\n",
    "\n",
    "- **Web scraping can be painfully slow**\n",
    "  - But it still can run in background\n",
    "\n",
    "- **Websites change frequently**\n",
    "  - You may need to update scripts\n",
    "\n",
    "- **Different websites will have diffferent structures**\n",
    "  - Every site is different and your script for one website may look very different from a script for another one\n",
    "\n",
    "- **Requires thorough HTML inspection**\n",
    "  - Try to find patterns to locate elements\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
